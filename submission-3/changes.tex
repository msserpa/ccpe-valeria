\documentclass{article}
%\usepackage{soul,xcolor}
\usepackage{xspace}

%Fancy usepackage to Xcolor to enable fancy colors such as ForestGreen
\usepackage[dvipsnames]{xcolor}

\usepackage{comment}

\title{Responses to reviewers}

\author{Valéria S. Girelli\\Francis B. Moreira\\Matheus S. Serpa\\Danilo Carastan-Santos\\Philippe O. A. Navaux}

\date{August 2020}

\pdfminorversion=4

\newcounter{answer}
\newenvironment{answer}
{ \refstepcounter{answer}\vspace{0.5cm}\bfseries\noindent Comment~\theanswer\\ }
{ \vspace{0.5cm} }

\newcommand{\ms}[1]{\textcolor{orange}{\textbf{ msserpa: #1} }\vspace{0.2cm}}
\newcommand{\vsg}[1]{\textcolor{blue}{\textbf{vsgirelli: #1} }\vspace{0.2cm}}
\newcommand{\dcs}[1]{\textcolor{ForestGreen}{\textbf{dcsantos: #1} }\vspace{0.2cm}}
\newcommand{\fbm}[1]{\textcolor{red}{\textbf{fbm: #1} }\vspace{0.2cm}}
\sloppy

\begin{document}


\maketitle

We thank the reviewers for their comments, which helped to improve the quality of the paper.
In this document, we included the comments of the reviewers and how we addressed each issue (formatted in \textbf{bold}).

% Editor feedback:

% The reviewers pointed to some issues that must be addressed in the paper, especially regarding the paper contributions, and the applicability of the obtained results. 

%You should upload a new version of your manuscript up to September 6, 2020.


% teoria
%         CG (FB) -> 15/08 (WIP, já vimos que IPC está alterado devido a perf pegando dados de outra thread q foi mapeada)
%             1. Primeiro problema notado foi que perf com -a -A pega estatísticas de outras threads/processos (não usamos nice no paper!)
%             2. Ao mudarmos para uso do threadload(PAPI), conseguimos isolar os contadores, e com a política OMP\_WAIT\_POLICY=ACTIVE ou undefined, a diferença de 110 instruções é fixa.
%             3. Porém, quando tentamos mudar para a política OMP\_WAIT\_POLICY=PASSIVE (a fim de evitar busy\_wait e as instruções adicionais), o número de instruções aumentou MUITO.
%             4. Construímos a hipótese de que a syscall de sleep da política passiva continha muitas instruções, e que devido à syscall estar linkada com vdso ao espaço do programa, o PAPI contava as instruções/ciclos de sleep, gerando o overhead.
%             5. Testamos em 2 partes: a) instruções/ciclos de sleep são contados pelo PAPI, b) CG com OMP\_WAIT\_POLICY=PASSIVE acarreta em sleep
%             a) CONFIRMADO: laço de 5 iterações com sleep e sem sleep acarreta em uma diferença de milhões de instrução
%             b) UNCONFIRMED: cg com passive NÃO FAZ SLEEP!
%             ??? oq o openmp faz com OMP\_WAIT\_PASSIVE??
%                 n tem function call a mais. (testado com ltrace)
%                 n tem syscall que indique algo a mais. (testado com strace)
                
            
%             WIP: rerodar INST e CYCLES com PAPI para reproduzir IPC e mostrar que gráfico estava errado
            
            
            
%             INSIGHT: PAPI >>>> PERF, não usar PERF!

% prática
%     REAL: (com relação aos pontos i, ii e iii (CG))
%       verificar se estava ativado -> 07/08 (tava) -- DONE
%       NAS (W, A, B) -- Turbo desativo - real - cei -> DONE
%       fazer map dos counters do PAPI -> DONE
%       colocar pra rodar e gerar novos resultados do real, comparar com resultados existentes na versão atual do paper -> ALMOST
       
%     SIMULADO: (com relação ao ponto i)
%         Sniper: L2, L1+L2, none -> 28/08 -> almost done
%         ZSim: só com none -> 31/08 ->
       
% apps do rodinia:
%         Breadth-First Search 
%         streamcluster
%         knn
%         kmeans -> I/O no laço principal 
%         backprop
%         b+tree
%         Pathfinder
        
% Reviews:
% i) ALMOST - reexecutamos tudo (respeitando o ponto ii), pros tamanhos W, A e B, pra mostrar a escalabilidade de acordo com a mudança do tamanho, e também estamos simulando com o tamanho W

% ii) OK - desativamos turbo boost, tudo executado

% iii) ONGOING - sobre Cg

% iv) ONGOING - other benchmarks


        
% https://github.com/pjreddie/darknet
% http://ai-benchmark.com/

% https://github.com/ParBLiSS/PaSGAL
% https://github.com/jshun/ligra

% https://github.com/YSZhuoyang/parallel-random-forests
% https://github.com/siddhantkulkarni/parallelmachinelearning
        
\section*{Reviewer 1}

Although the manuscript has a good structure, the reviewer feels that the lack of novelty and innovative research contents are major concerns. In particular, the paper presents neither an architecture nor an innovative prefetch algorithm to improve its role in the performance of HPC applications. 

\begin{answer}
%DCS: Mentir, dizendo que foi uma observação bastante imporatnte, que revelou que um aspecto de importância crítica do artigo não estava apropriadamente claro no texto.
%We understand and respect the reviewer's opinion. It was an important observation, which made us notice that \ms{não curti muito essa frase} perhaps one of our paper's critical aspects is not adequately clear in the manuscript. We elaborate on this crucial aspect in the following paragraphs.

We understand and respect the reviewer's opinion. It was an important observation, which made us notice that perhaps one of our paper's critical aspects is not adequately clear in the manuscript. We elaborate on this crucial aspect in the following paragraphs.

%Although some manufacturers provide clear definitions concerning their architectures and prefetcher algorithms, there is a noticeable amount of concealment regarding the architectures and prefetcher algorithms, \ms{cuidado} especially the ones provided by Intel. These obscurities not only hinder the development of accurate architecture simulators, though also leave us with the only choice of performing experimental research to shed light on the understanding of what happens with the performance of parallel applications when one processes them in modern architectures with prefetcher algorithms. 

%We understand that our paper is an alert to developers ....  that the simulators don't presents correctly what happen in a real program, ......  that is important that simulators developers need to improved then to better represents the processors' behavior .....   algo assim

\vsg{I'm not sure if I like the term "alert to developers". "Alert to architects" maybe? "investigation to architects"?}
We envision our paper as an alert to developers. Architecture simulators are a vital component for developing new prefetcher algorithms and for studies over the memory hierarchy. We show experimental evidence that the simulators do not correctly represent real architectures, specifically when we consider memory prefetcher and parallel applications. Improving simulators' accuracy is a concerning and essential matter for the future of architecture research.

\vsg{I found this first sentence kinda contradictory. First we say that some manufacturers provide clear definitions about their archs and pref algs, and then we say the opposite. Or am I confused?}
Although some manufacturers provide clear definitions concerning their architectures and prefetcher algorithms, there is a noticeable amount of concealment regarding the architectures and prefetcher algorithms. These obscurities not only hinder the development of accurate architecture simulators, though also leave us with the only choice of performing experimental research to shed light on the understanding of what happens with the performance of parallel applications when one processes them in modern architectures with prefetcher algorithms.
\vsg{and it is also important to mention that very few works exist over parallel architecture prefetchers. Our work is important for helping architects to develop new prefetching algorithms on parallel architectures because we can identify error sources, performance bottlenecks, etc.}

Bearing this in mind, we sought to conceive a paper that encapsulates all the experiences we obtained when investigating memory prefetcher algorithms over parallel applications, accounting for real executions and simulations. 
We found many essential aspects of the prefetchers' behavior over parallel applications, notably its efficiency drop when parallelism increases. We also perceived many critical specificities of the ZSim and Sniper simulators, when one uses these to simulate the execution of parallel applications, taking into account the prefetcher when possible. 
\vsg{I guess we can be more direct here mentioning that we were able to identify a few errors, for example, the small usefulness of the prefetches being issued. We not only show that the simulators don't present the expected performance, but we also show "proofs" of their error sources. This is something that may have a direct effect on other people's research.}

In the revised version, we added a new section (Section 6.1) to strengthen the aspect mentioned above, which emphasizes on providing guidelines, with technical details when pertinent. We are confident that the findings presented by the manuscript can give reliable guidance as to which practices one can follow to research and develop architectures, simulators, and prefetcher algorithms. 



%DCS: Mencionar alguns highlights das observações mais importantes do paper
%We are confident that, with the findings presented by the paper, we can provide helpful information and insights to guide future architecture and prefethcer algorithms practices and resarch, on both real and simulated scenarios. %DCS: Aprimorar mais essa úlima frase, dizendo que pode também guiar pesquisa em simuladores de arquitetura

%\ms{pensar muito bem em como responder. é a resposta mais importante dessa revisão}
%\ms{falar as contribuições}


%\vsg{Danilo sugere que esse nem era o ponto do artigo, então é bom comentar isso}
%\vsg{existem prefetchers que tentam justamente lidar com problemas causados por inter-core etc}
%\vsg{é uma revista de practice and experience}
%\ms{cuidado com esse tipo de resposta :)}

\end{answer}

\section*{Reviewer 2}

i) The performance described in Figure 2 and the next ones consider a fixed-size problem over the thread count increment (like in a strong-scaling evaluation)? If it is correct, a weak scaling performance analysis could also be added, which could help to find the reason for the performance decreasing. The paper suggests this effect occurs due to the possible memory contention in a multithread execution, but the difference in the problem granularity could also be affecting.

\begin{answer}
%\ms{fazer experimentos com diferentes tamanhos do NAS (talvez menor ai é mais rápido no simulador)}
%\vsg{ver se ta bem especificado no texto o tamanho do problema e que é mantido fixo}

%\dcs{Rodando com o W, o IPC do IS aumentou (mais ou menos como o CG), mas o IPC do CG não aumentou da mesma maneira que com o A. Tirando isso as demais apps se comportaram mais ou menos da mesma maneira (IPC caindo). O B me parece mais interessante, pq o IPC de todo mundo cai mais rápido (inclusive o CG!). Acho que adicionar o B ao invés do W ficaria mais interessante no paper.}

We thank the reviewer for the comment. All of the real and simulated experiments were executed using the input size class A of the NAS benchmark. The A class situates on the lower end of the available input sizes. From the smallest to the largest, we have classes S, W, A, B, C, D, E, and F. We chose class A because it was the class that enabled us to perform simulations with Sniper in reasonable time.

As cleverly pointed out, we agree that the input size can also be affecting the performance. With this in mind, in the revised version of the manuscript we added experiments taking into the account the input size class W, A, and B of the NAS benchmark. These new experiments can be found in Section 5.1.2 of the revised manuscript. We also added information regarding to the NAS input size classes on last paragraph of Section 4.2.
\end{answer}

ii) What is the clock frequency used in all real executions? The processor Turbo frequency was disabled?

\begin{answer}
%dcs: executamos com turbo boost on e off com o PAPI e não notamos diferença no IPC
%dcs: o lance com o perf eh que ele coleta os dados system-wide, então dados de outros processos do sistema podem interferir
%dcs: portanto a gente trocou de perf para PAPI, que coleta os dados application-wide
%dcs: a verdade foi com o turbo boost ON, mas falar que foi off não muda nada, já que a gente vai trocar todos os gráficos praticamente

We thank the reviewer for the comment. All executions were performed with the Intel Turbo Boost off, with a base clock frequency of 2.1 GHz. However, we noticed some anomalies in the experiments, notably high variabilities (error bars) in some applications for the IPC. We later found out that the Perf profiling tool collects data in a system-wide manner, meaning that system processes may interfere in the results. We decided therefore to rerun the experiments using the PAPI profiling tool, since PAPI provides more control over the measurements and, most importantly, it collects performance data application-wide, and not system-wide. We updated all figures that showed data about real executions with the updated results using PAPI. We added the information about Turbo Boost and clock frequency in Section 4.1, and the information about PAPI and Perf in Section 6.1.
%\ms{refazer tudo com turbo boost desligado e comparar com o que enviamos (turbo boost ativado)}

%\ms{lorenzon tem experimentos que mostram que para algumas aplicações da bastante diferença (principalmente nos ciclos)}

%\ms{lorenzon: se ela é cpu-bound, os ganhos são bons}
%\ms{lorenzon: se é memory-bound, depende da parte sequencial}
%\ms{lorenzon: mas o consumo de energia, no geral, sempre aumenta.}



\end{answer}

iii) on page 11, it is stated that the prefetching performance decrease is due to irregular data access in memory. A detailed explanation about the CG-NPB could help to understand why this happened

\begin{answer}
\ms{francis}

Blablabla

We added this information to the last paragraph of Section~x.y.
\end{answer}

iv) The conclusion recommends special attention to highly parallel applications, which may benefit from turning off the prefetcher. The benchmark used comprises just parallel numerical models. Nowadays, other kinds of highly parallel applications are very common, like the ones used for graphs or Machine Learning. The recommendation could also be extended to any kind of highly parallel apps?

\begin{answer}
%\ms{outros benchmarks -- talvez rodinia (esse tem ML e grafos) ou spec omp?}
%\dcs{Vamos ver se teremos tempo para colocar o rodinia-backprop. Senão a gente inventa uma resposta, generalizando as observações para outras apps. Dá pra responder, por exemplo, quais características uma app precisa ter para que os resultados que obtivemos sejam similares}

We thank the reviewer for the question. During our investigation, we generalized a set of application characteristics, that our recommendation can be extended, namely (i) for any non-embarrassingly parallel application, and (ii) for embarrassingly parallel applications with in-place memory operations. Many parallel applications are non-embarrassingly parallel, since embarrassingly parallel preventing operations such as locking and synchronization are often required in parallel algorithms (e.g. parallel Map Reduce or Stochastic Gradient Descent) and embarrassingly parallel applications with in-place memory operations are less common, such as the pseudo-random number generation performed by EP-NPB. The exceptions to our recommendation are embarrassingly parallel applications that operate over vectors/matrices, such as parallel vector sum or matrix multiplication. these applications do not suffer from contention caused by synchronization or locking, and may benefit from prefetchers, since the number of memory accesses 


We added this information to the last paragraph of Section~x.y.
\end{answer}


\end{document}
